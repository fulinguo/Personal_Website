<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Personal Website</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>Fulin Guo</h1>
        <nav>
            <ul>
                <!--  <li><a href="#research">Research</a></li>
                <li><a href="#cv">CV</a></li> -->
            </ul>
        </nav>
    </header>

    

    <section id="about">
        <h2>About Me</h2>
        <img src="fulinguo_photo.jpg" alt="Fulin Guo" id="profile-photo"> 
        <p>I am Fulin Guo, a PhD candidate in Economics at the University of Cambridge.</p>

        <p>My research interests are in AI and machine learning in economics, networks, and computational social science.</p>
        
        <p> Prior to my PhD at the University of Cambridge, I had a MA in Computational Social Science from the University of Chicago and BS in Economic Statistics from Huazhong University of Science and Technology.</p>

        <p>My CV is <a href="https://drive.google.com/file/d/1wyoj-7PT_RHFtad3MeHIHsIFtP9DVRR8/view?usp=sharing" target="_blank">here</a>.</p>

        <!-- Add more about yourself here -->
    </section>

    <div style="clear: both;"></div>
    
    </section id="research">
        <h2>Research</h2>
        <!-- add bold text-->
        <p> GPT in Game Theory Experiments [<a href="https://arxiv.org/abs/2305.05516" target="_blank">PDF</a>]</p>
        <p> This paper explores the use of Generative Pre-trained Transformers (GPT) in strategic game
            experiments, specifically in the finitely repeated play of the ultimatum game and of the prisoner's
            dilemma. I designed prompts and architectures to enable GPT to understand the game rules and
            to generate both its choices and the reasoning behind decisions. The key findings show that GPT
            exhibits behaviors similar to those of humans in important aspects, such as making positive offers
            and rejecting unfair ones in the ultimatum game, along with conditional cooperation strategy in
            the prisoner's dilemma. The study explores how prompting GPT with traits of fairness concern or
            selfishness influences its decisions. Notably, the “fair” GPT in the ultimatum game tends to make
            higher offers and reject offers more frequently compared to the “selfish” GPT. In the prisoner's
            dilemma, the “fair” GPT has a significantly higher cooperation rate than the “selfish” GPT. The
            reasoning statements produced by GPT during gameplay uncover the underlying logic of certain
            intriguing patterns observed in the games. They also reveal that GPT's cooperative behaviors
            in the prisoner's dilemma are sometimes driven, at least in part, by errors in reasoning. Overall,
            this research shows the potential of GPT as a valuable tool in social science research, especially
            in experimental studies and social simulations.</p>
        </ul>
<!--
    <section id="contact">
        <h2>Contact</h2>
        <p>Email: fg400@cam.ac.uk; Phone: +44 7529275181</p>
    </section>
-->
    <footer>
        <p>Email: fg400@cam.ac.uk; Phone: +44 7529275181</p>
        <p>&copy; 2024 Fulin Guo. All rights reserved.</p>
    </footer>

    <script src="script.js"></script>
</body>
</html>
